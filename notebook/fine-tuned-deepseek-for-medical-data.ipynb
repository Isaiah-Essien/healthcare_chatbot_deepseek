{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":267841,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":229219,"modelId":250963}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning DeepSeek Distilled llama 3.2 for medical conversation","metadata":{}},{"cell_type":"markdown","source":"\n**This project involves fine-tuning a Large Language Model (LLM) DeepSeek-R1-Distilled-Llma3.2 using LoRA (Low-Rank Adaptation) and Unsloft for efficient training and inference. The model is designed to generate responses for health-related queries, specifically focusing on medical diagnosis-related prompts. The fine-tuned model is based on the DeepSeek-R1-Distill-Llama-8B architecture, and it has been optimized for fast inference using 4-bit quantization. The training process leverages supervised fine-tuning (SFT) techniques to adapt the base model to the specific task.**\n\n- [Link to Model:](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)\n- [Link to dataset:](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)\n","metadata":{}},{"cell_type":"markdown","source":"# Techniques Used\n### **1. LoRA (Low-Rank Adaptation)**\nLoRA is a parameter-efficient fine-tuning technique that allows adaptation of a large pre-trained model without modifying the entire weight matrix. Instead of updating the full model weights, LoRA adds trainable low-rank matrices, significantly reducing memory usage and training costs.\n\n- **Advantage**: It allows fine-tuning large models like LLaMA efficiently on limited computational resources.\n- **Implementation**: Integrated into **Unsloft** for efficient parameter updates.\n\n### **2. Unsloft**\n**Unsloft** is a library optimized for **efficient model fine-tuning and inference**. It integrates **FastLanguageModel**, which is used to:\n- Load and configure pre-trained models with quantization.\n- Apply custom chat templates for structured prompt formatting.\n- Enable **2x faster inference** by optimizing token generation.\n\n### **3. 4-bit Quantization**\nQuantization is used to reduce the memory footprint of large models, making them more efficient for deployment. This project uses **4-bit quantization**, allowing the model to run on lower-end GPUs with minimal performance loss.\n\n# Data Preprocessing Steps\n1. **Dataset Loading**: The dataset is loaded using the **Hugging Face `datasets` library**.\n2. **Formatting for Chat Model**:\n   - The dataset is structured into user-assistant message pairs.\n   - Custom chat templates from **Unsloft** are applied.\n3. **Tokenization**:\n   - Input sequences are tokenized using the **LLaMA tokenizer**.\n   - The `max_seq_length` is set to **2048 tokens**.\n   - **Padding and truncation** are applied to maintain consistent sequence lengths.\n4. **Training Data Collation**:\n   - **`DataCollatorForSeq2Seq`** is used for batching and formatting data.\n   - Ensures efficient handling of different sequence lengths.\n\n# Training Configuration & Parameters\nThe model is trained using **Supervised Fine-Tuning (SFT)** with the following configurations:\n\n- **Model Architecture**: `DeepSeek-R1-Distill-Llama-8B`\n- **Training Arguments**:\n  - `max_seq_length = 2048`  (Maximum token length per input)\n  - `dtype = None`  (Default datatype for efficient computation)\n  - `load_in_4bit = True`  (Enables 4-bit quantization for faster training and inference)\n  - `use_cache = True`  (Stores previous token predictions for efficiency)\n  - `temperature = 0.6`  (Controls randomness in response generation)\n  - `min_p = 0.1`  (Sets minimum probability threshold for token selection)\n\n# Model Inference\nAfter training, the model is tested by:\n1. Using a structured **chat template** to format user queries.\n2. Applying **tokenization** with **padding and truncation**.\n3. Generating responses using `model.generate()`, with:\n   - `max_new_tokens = 64` (Restricts output length)\n   - `use_cache = True` (Optimizes generation speed)\n\n# Summary\nThis notebook successfully implements a **fine-tuned LLaMA-based chatbot** optimized for **medical diagnosis-related queries** using **LoRA and Unsloft**. The use of **quantization, structured training, and efficient inference** makes the model practical for real-world applications where computational efficiency is critical. The responses are not top notch, but they are great considering the model was trained for only 2 epochs which took 12 hours. \nThe model is evaluated on perplexity abd Blue. Both may not give great scores, but it is better than nothing.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Installations","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install unsloth==2025.2.4\n!pip install unsloth_zoo==2025.2.3\n!pip install torch==2.5.1\n!pip install torchaudio==2.5.1\n!pip install torchvision=0.20.1\n!pip install vllm==0.7.2\n!pip install xformers==0.0.28.post3\n!pip install xgrammar==0.1.11\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install trl==0.8.2\n!pip install streamlit -q\n!pip install pyngrok","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:30:27.447876Z","iopub.execute_input":"2025-02-25T08:30:27.448196Z","iopub.status.idle":"2025-02-25T08:35:53.431425Z","shell.execute_reply.started":"2025-02-25T08:30:27.448172Z","shell.execute_reply":"2025-02-25T08:35:53.430173Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"#Imports\nimport unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nfrom  unsloth.chat_templates import get_chat_template\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom unsloth.chat_templates import get_chat_template\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments,DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\nfrom unsloth.chat_templates import train_on_responses_only\n# import streamlit as st\n# from datasets import load_metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:35:53.432909Z","iopub.execute_input":"2025-02-25T08:35:53.433206Z","iopub.status.idle":"2025-02-25T08:36:29.938011Z","shell.execute_reply.started":"2025-02-25T08:35:53.433180Z","shell.execute_reply":"2025-02-25T08:36:29.937152Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Patching Xformers to fix some performance issues.\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Data Exploration and Preprocessing combined","metadata":{}},{"cell_type":"code","source":"#Load the original model and tokenizer\n\n#Define configuraions for loading the model\n\nmax_seq_length=2048\ndtype=None\nload_in_4bit=True\n\nmodel,tokenizer=FastLanguageModel.from_pretrained(\n    model_name='unsloth/DeepSeek-R1-Distill-Llama-8B',\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:34:38.041404Z","iopub.execute_input":"2025-02-24T20:34:38.041746Z","iopub.status.idle":"2025-02-24T20:35:05.203614Z","shell.execute_reply.started":"2025-02-24T20:34:38.041717Z","shell.execute_reply":"2025-02-24T20:35:05.202956Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b9f1187fad4ec088679f23449f68c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770aaf2d64ea4ad8b697026fcff2e331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4df6da0bf1b4b71a082af56f4862cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"084f0073331241cb98b5a899012f101c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c048a4cd544873933429967e69d091"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Test the original model with some Health diagnoses.\ntokenizer=get_chat_template(\n    tokenizer,\n    chat_template='llama-3.2',\n)\n\n#Set the PAD to be the same as the EOS token and avoid tokenization issues\ntokenizer.pad_token=tokenizer.eos_token\nFastLanguageModel.for_inference(model) #enable native 2x faster inference\n\nmessages=[\n    {'role':'user','content':'I have a headache, a bad one around the forehead'}\n]\n#Tokenize the user input with the chat template\n\ninputs=tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors='pt',\n    padding=True,# Add padding to match sequence length\n\n).to('cuda')\n\nattention_mask=inputs!=tokenizer.pad_token_id\n#Generate the output\noutputs=model.generate(\n    input_ids=inputs,\n    attention_mask=attention_mask,\n    max_new_tokens=64,\n    use_cache=True, #use cache for faster token generation\n    temperature=0.6, # controls randomness in response\n    min_p=0.1 #Sets the minimum probability threshold for token selection\n)\n\n#Decode the generated tokens into human-readable text\ntext=tokenizer.decode(outputs[0],skip_special_tokens=True)\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:35:10.790194Z","iopub.execute_input":"2025-02-24T20:35:10.790542Z","iopub.status.idle":"2025-02-24T20:35:19.188278Z","shell.execute_reply.started":"2025-02-24T20:35:10.790515Z","shell.execute_reply":"2025-02-24T20:35:19.187458Z"}},"outputs":[{"name":"stdout","text":"system\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\nuser\n\nI have a headache, a bad one around the foreheadassistant\n\nI'm sorry to hear that you're feeling unwell. If your headache is persistent, I recommend consulting a healthcare professional for advice and treatment. If you'd like, I can also provide some tips that might help alleviate your symptoms. Please let me know how I can assist you further.\n</think>\n\nIt seems like you\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Applying low rank","metadata":{}},{"cell_type":"code","source":"#Apply LoRA adapters for Efficient Fine-tuning\n\nmodel =FastLanguageModel.get_peft_model(\n    model,\n    r=16,#LoRA rank controls low-rank approximation quality\n    target_modules=['q_proj','v_proj','k_proj','o_proj','gate_proj','up_proj','down_proj'],#Layers to apply in LoRA\n    lora_alpha=16, #Scaling factor for loRA weights\n    lora_dropout=0,\n    bias='none',\n    use_gradient_checkpointing=True,\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None\n\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:35:30.462771Z","iopub.execute_input":"2025-02-24T20:35:30.463088Z","iopub.status.idle":"2025-02-24T20:35:33.863278Z","shell.execute_reply.started":"2025-02-24T20:35:30.463066Z","shell.execute_reply":"2025-02-24T20:35:33.862574Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Spliting the train data and using only 3,500 rows for a bit faster training ","metadata":{}},{"cell_type":"code","source":"#Prepare the dataset\n\n#load data\ndataset=load_dataset('FreedomIntelligence/medical-o1-reasoning-SFT','en', split='train')\ndataset= dataset.select(range(3_500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:35:46.728307Z","iopub.execute_input":"2025-02-24T20:35:46.728774Z","iopub.status.idle":"2025-02-24T20:35:50.662887Z","shell.execute_reply.started":"2025-02-24T20:35:46.728734Z","shell.execute_reply":"2025-02-24T20:35:50.662011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecfb7c31ca6a45f5b621e349436c19ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"medical_o1_sft.json:   0%|          | 0.00/74.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e3a5e069e84effa59ecb7715eef14a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25371 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed167ad1fda242999ec4dfd54ee6d23d"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Converting the dataset to Hugging face Role, Content format\n","metadata":{}},{"cell_type":"code","source":"\ndataset_list = dataset.to_list()\n\n# Transform to Hugging Face Standard Role-Content Format\nformatted_dataset = [\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": data[\"Question\"]},   # User input\n            {\"role\": \"assistant\", \"content\": data[\"Complex_CoT\"]},  # AI thinking process\n            {\"role\": \"assistant\", \"content\": data[\"Response\"]}  # Final AI response\n        ]\n    }\n    for data in dataset_list  # Iterate over each row in dataset\n]\n\n# Convert back to Hugging Face Dataset format\nhf_dataset = Dataset.from_list(formatted_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:08.084684Z","iopub.execute_input":"2025-02-24T20:37:08.085018Z","iopub.status.idle":"2025-02-24T20:37:08.173454Z","shell.execute_reply.started":"2025-02-24T20:37:08.084994Z","shell.execute_reply":"2025-02-24T20:37:08.172779Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Checking the firts row of the dataset","metadata":{}},{"cell_type":"code","source":"hf_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:12.484816Z","iopub.execute_input":"2025-02-24T20:37:12.485162Z","iopub.status.idle":"2025-02-24T20:37:12.490999Z","shell.execute_reply.started":"2025-02-24T20:37:12.485132Z","shell.execute_reply":"2025-02-24T20:37:12.490308Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'content': 'A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?',\n   'role': 'user'},\n  {'content': \"Okay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \\n\\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\\n\\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \\n\\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.\",\n   'role': 'assistant'},\n  {'content': 'Cystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Extracting conversation to a structured format for training","metadata":{}},{"cell_type":"code","source":"# Function to extract conversation as a structured format\ndef format_prompts(examples):\n    convos = examples[\"messages\"]  # Extract the messages list\n\n    # Apply the tokenizer template to the conversation\n    texts = [\n        tokenizer.apply_chat_template(\n            convo,\n            tokenize=False,\n            add_generation_prompt=False\n        )\n        for convo in convos\n    ]\n\n    return {\"text\": texts}\n\n# Apply the function using .map()\nhf_dataset = hf_dataset.map(format_prompts, batched=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:15.272124Z","iopub.execute_input":"2025-02-24T20:37:15.272429Z","iopub.status.idle":"2025-02-24T20:37:15.829286Z","shell.execute_reply.started":"2025-02-24T20:37:15.272406Z","shell.execute_reply":"2025-02-24T20:37:15.828186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb4f29551a541fca95835cea559cad6"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"print(hf_dataset[0][\"messages\"][0][\"content\"])\nprint(hf_dataset[0]['text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:19.140903Z","iopub.execute_input":"2025-02-24T20:37:19.141358Z","iopub.status.idle":"2025-02-24T20:37:19.147769Z","shell.execute_reply.started":"2025-02-24T20:37:19.141317Z","shell.execute_reply":"2025-02-24T20:37:19.146619Z"}},"outputs":[{"name":"stdout","text":"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\n<｜begin▁of▁sentence｜><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nA 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nOkay, let's think about this step by step. There's a 61-year-old woman here who's been dealing with involuntary urine leakages whenever she's doing something that ups her abdominal pressure like coughing or sneezing. This sounds a lot like stress urinary incontinence to me. Now, it's interesting that she doesn't have any issues at night; she isn't experiencing leakage while sleeping. This likely means her bladder's ability to hold urine is fine when she isn't under physical stress. Hmm, that's a clue that we're dealing with something related to pressure rather than a bladder muscle problem. \n\nThe fact that she underwent a Q-tip test is intriguing too. This test is usually done to assess urethral mobility. In stress incontinence, a Q-tip might move significantly, showing urethral hypermobility. This kind of movement often means there's a weakness in the support structures that should help keep the urethra closed during increases in abdominal pressure. So, that's aligning well with stress incontinence.\n\nNow, let's think about what would happen during cystometry. Since stress incontinence isn't usually about sudden bladder contractions, I wouldn't expect to see involuntary detrusor contractions during this test. Her bladder isn't spasming or anything; it's more about the support structure failing under stress. Plus, she likely empties her bladder completely because stress incontinence doesn't typically involve incomplete emptying. So, her residual volume should be pretty normal. \n\nAll in all, it seems like if they do a cystometry on her, it will likely show a normal residual volume and no involuntary contractions. Yup, I think that makes sense given her symptoms and the typical presentations of stress urinary incontinence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nCystometry in this case of stress urinary incontinence would most likely reveal a normal post-void residual volume, as stress incontinence typically does not involve issues with bladder emptying. Additionally, since stress urinary incontinence is primarily related to physical exertion and not an overactive bladder, you would not expect to see any involuntary detrusor contractions during the test.<|eot_id|>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Setting up parameters for training","metadata":{}},{"cell_type":"code","source":"#Define training configurations\n\ntrainer=SFTTrainer(\n    model=model,\n    train_dataset=hf_dataset,\n    tokenizer=tokenizer,\n    dataset_text_field='text',\n    max_seq_length=max_seq_length,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n    dataset_num_proc=2,\n    packing=False,\n\n    args=TrainingArguments(\n        per_device_train_batch_size=2,#Number of Examples per GPU batch\n        gradient_accumulation_steps=4,#Accumulate gradients over 4 batches before updating model\n        warmup_steps=5, #Number of warm up steps for lr schedule\n        max_steps=-1, #Total number of training steps\n        num_train_epochs=2,\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,#Logs the training metrics after every step\n        optim='adamw_8bit',\n        weight_decay=0.01,\n        lr_scheduler_type='linear',#Linear decay for learning rate\n        seed=3407,\n        output_dir='outputs',#Directory to save model checkpoints\n        report_to='none'\n\n    )\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:24.103368Z","iopub.execute_input":"2025-02-24T20:37:24.103714Z","iopub.status.idle":"2025-02-24T20:37:31.089674Z","shell.execute_reply.started":"2025-02-24T20:37:24.103684Z","shell.execute_reply":"2025-02-24T20:37:31.088477Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/3500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e62d0935c346bb98abcb25f37a059e"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Train! Train! Train!","metadata":{}},{"cell_type":"code","source":"#To improve Efficiency, I will train only on Responses rather than user inputs\ntrainer=train_on_responses_only(\n    trainer,\n    instruction_part='<|start_header_id|>user<|end_header_id|>\\n\\n',#mark user input\n    response_part='<|start_header_id|>assistant<|end_header_id|>\\n\\n',#mark AI response\n)\n\n#Start training the model\ntrainer_stats=trainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:37:35.761233Z","iopub.execute_input":"2025-02-24T20:37:35.761563Z","iopub.status.idle":"2025-02-25T06:34:48.535659Z","shell.execute_reply.started":"2025-02-24T20:37:35.761529Z","shell.execute_reply":"2025-02-25T06:34:48.534481Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad19a1147684f48b54ce6f409598bf7"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 3,500 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 874\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='874' max='874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [874/874 9:56:29, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.118200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.717000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.974800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.159500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.825700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.819600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.880600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.663400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.566800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.690100</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.622200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.707400</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.653900</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.725300</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.744200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.419300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.564300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.392200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.655000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.662400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.631100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.485300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.538900</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.548300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.471100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.540500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.452200</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.602400</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.245900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.436200</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.709400</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.477000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.293600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.233500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.602700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.530800</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.367900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.395000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.559600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.333400</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.331100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.354900</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.498400</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.572900</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.409100</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.338900</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.349400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.449700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>1.511700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.413300</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>1.389900</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.392200</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.481000</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.472200</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.567400</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.335600</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.635600</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>1.534900</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>1.471900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.452200</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.470400</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.395400</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.410700</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.600400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.397400</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>1.469800</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.541500</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.492800</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.384500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.360000</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>1.429000</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.504000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.464000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.472700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.524500</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>1.400600</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>1.510100</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.304500</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.417700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.549100</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.358900</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>1.441600</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.262200</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>1.581000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.503200</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>1.199800</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>1.553300</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>1.317800</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>1.419200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.394900</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.374400</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.474500</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.169300</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.445100</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.417700</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.397900</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>1.386800</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>1.383500</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.185500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.554300</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.394300</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>1.536200</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>1.385200</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>1.448600</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.476500</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.365600</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>1.084700</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>1.245800</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>1.255600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.400100</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>1.530500</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>1.463000</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>1.464900</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>1.581700</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.424700</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>1.445700</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>1.548400</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>1.401400</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>1.374300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.282600</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>1.254000</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>1.421900</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>1.424400</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.495600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.445700</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>1.282800</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>1.451600</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>1.411000</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>1.399000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.371700</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>1.330000</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>1.368200</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>1.297400</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>1.540900</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.253700</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>1.461400</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>1.398500</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>1.425600</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>1.536100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.442700</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>1.471400</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>1.468100</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>1.697100</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>1.402500</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.376600</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>1.167000</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>1.423200</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>1.474100</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>1.655900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.431200</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>1.458900</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>1.502400</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>1.408800</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>1.372200</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.356400</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>1.203500</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>1.449400</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>1.394500</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>1.352700</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.329100</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>1.499000</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>1.319900</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>1.367700</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>1.411500</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.435100</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>1.469500</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>1.348500</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>1.318800</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>1.359300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.543800</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>1.431700</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>1.233000</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>1.425400</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>1.380000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.314100</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>1.352000</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>1.453100</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>1.571300</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>1.450100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.565800</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>1.514200</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>1.329400</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>1.317900</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>1.380500</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.342100</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>1.435600</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>1.450300</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>1.484200</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>1.359500</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.522800</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>1.498300</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>1.340600</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>1.561100</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>1.252300</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.502000</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>1.576200</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>1.320600</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>1.484400</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>1.271200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.401200</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>1.587000</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>1.423500</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>1.426000</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>1.475700</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>1.398300</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>1.688400</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>1.434000</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>1.450100</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>1.490000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.462700</td>\n    </tr>\n    <tr>\n      <td>211</td>\n      <td>1.409200</td>\n    </tr>\n    <tr>\n      <td>212</td>\n      <td>1.370100</td>\n    </tr>\n    <tr>\n      <td>213</td>\n      <td>1.389200</td>\n    </tr>\n    <tr>\n      <td>214</td>\n      <td>1.401100</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>1.470300</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>1.441900</td>\n    </tr>\n    <tr>\n      <td>217</td>\n      <td>1.253600</td>\n    </tr>\n    <tr>\n      <td>218</td>\n      <td>1.368300</td>\n    </tr>\n    <tr>\n      <td>219</td>\n      <td>1.457000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.353200</td>\n    </tr>\n    <tr>\n      <td>221</td>\n      <td>1.342500</td>\n    </tr>\n    <tr>\n      <td>222</td>\n      <td>1.534500</td>\n    </tr>\n    <tr>\n      <td>223</td>\n      <td>1.416500</td>\n    </tr>\n    <tr>\n      <td>224</td>\n      <td>1.462000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.488800</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>1.512900</td>\n    </tr>\n    <tr>\n      <td>227</td>\n      <td>1.430100</td>\n    </tr>\n    <tr>\n      <td>228</td>\n      <td>1.288700</td>\n    </tr>\n    <tr>\n      <td>229</td>\n      <td>1.452600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.230500</td>\n    </tr>\n    <tr>\n      <td>231</td>\n      <td>1.365100</td>\n    </tr>\n    <tr>\n      <td>232</td>\n      <td>1.379500</td>\n    </tr>\n    <tr>\n      <td>233</td>\n      <td>1.484500</td>\n    </tr>\n    <tr>\n      <td>234</td>\n      <td>1.413000</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>1.409000</td>\n    </tr>\n    <tr>\n      <td>236</td>\n      <td>1.304600</td>\n    </tr>\n    <tr>\n      <td>237</td>\n      <td>1.137500</td>\n    </tr>\n    <tr>\n      <td>238</td>\n      <td>1.410100</td>\n    </tr>\n    <tr>\n      <td>239</td>\n      <td>1.275900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.291000</td>\n    </tr>\n    <tr>\n      <td>241</td>\n      <td>1.460400</td>\n    </tr>\n    <tr>\n      <td>242</td>\n      <td>1.308700</td>\n    </tr>\n    <tr>\n      <td>243</td>\n      <td>1.328600</td>\n    </tr>\n    <tr>\n      <td>244</td>\n      <td>1.405100</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>1.523000</td>\n    </tr>\n    <tr>\n      <td>246</td>\n      <td>1.436100</td>\n    </tr>\n    <tr>\n      <td>247</td>\n      <td>1.339700</td>\n    </tr>\n    <tr>\n      <td>248</td>\n      <td>1.397600</td>\n    </tr>\n    <tr>\n      <td>249</td>\n      <td>1.277100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.478300</td>\n    </tr>\n    <tr>\n      <td>251</td>\n      <td>1.258200</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>1.503500</td>\n    </tr>\n    <tr>\n      <td>253</td>\n      <td>1.364500</td>\n    </tr>\n    <tr>\n      <td>254</td>\n      <td>1.456600</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>1.460200</td>\n    </tr>\n    <tr>\n      <td>256</td>\n      <td>1.480700</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>1.441000</td>\n    </tr>\n    <tr>\n      <td>258</td>\n      <td>1.413400</td>\n    </tr>\n    <tr>\n      <td>259</td>\n      <td>1.370500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.417500</td>\n    </tr>\n    <tr>\n      <td>261</td>\n      <td>1.447600</td>\n    </tr>\n    <tr>\n      <td>262</td>\n      <td>1.316500</td>\n    </tr>\n    <tr>\n      <td>263</td>\n      <td>1.529600</td>\n    </tr>\n    <tr>\n      <td>264</td>\n      <td>1.320600</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>1.325300</td>\n    </tr>\n    <tr>\n      <td>266</td>\n      <td>1.258900</td>\n    </tr>\n    <tr>\n      <td>267</td>\n      <td>1.396700</td>\n    </tr>\n    <tr>\n      <td>268</td>\n      <td>1.492900</td>\n    </tr>\n    <tr>\n      <td>269</td>\n      <td>1.411400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.441100</td>\n    </tr>\n    <tr>\n      <td>271</td>\n      <td>1.449600</td>\n    </tr>\n    <tr>\n      <td>272</td>\n      <td>1.444600</td>\n    </tr>\n    <tr>\n      <td>273</td>\n      <td>1.308400</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>1.292900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.188300</td>\n    </tr>\n    <tr>\n      <td>276</td>\n      <td>1.332900</td>\n    </tr>\n    <tr>\n      <td>277</td>\n      <td>1.236600</td>\n    </tr>\n    <tr>\n      <td>278</td>\n      <td>1.441900</td>\n    </tr>\n    <tr>\n      <td>279</td>\n      <td>1.337000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.451800</td>\n    </tr>\n    <tr>\n      <td>281</td>\n      <td>1.287000</td>\n    </tr>\n    <tr>\n      <td>282</td>\n      <td>1.420500</td>\n    </tr>\n    <tr>\n      <td>283</td>\n      <td>1.381700</td>\n    </tr>\n    <tr>\n      <td>284</td>\n      <td>1.377200</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>1.424400</td>\n    </tr>\n    <tr>\n      <td>286</td>\n      <td>1.329500</td>\n    </tr>\n    <tr>\n      <td>287</td>\n      <td>1.317100</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>1.423900</td>\n    </tr>\n    <tr>\n      <td>289</td>\n      <td>1.485800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.512100</td>\n    </tr>\n    <tr>\n      <td>291</td>\n      <td>1.265200</td>\n    </tr>\n    <tr>\n      <td>292</td>\n      <td>1.457200</td>\n    </tr>\n    <tr>\n      <td>293</td>\n      <td>1.446000</td>\n    </tr>\n    <tr>\n      <td>294</td>\n      <td>1.368000</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>1.278900</td>\n    </tr>\n    <tr>\n      <td>296</td>\n      <td>1.427200</td>\n    </tr>\n    <tr>\n      <td>297</td>\n      <td>1.376100</td>\n    </tr>\n    <tr>\n      <td>298</td>\n      <td>1.456900</td>\n    </tr>\n    <tr>\n      <td>299</td>\n      <td>1.440400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.492200</td>\n    </tr>\n    <tr>\n      <td>301</td>\n      <td>1.335600</td>\n    </tr>\n    <tr>\n      <td>302</td>\n      <td>1.330600</td>\n    </tr>\n    <tr>\n      <td>303</td>\n      <td>1.346100</td>\n    </tr>\n    <tr>\n      <td>304</td>\n      <td>1.495200</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>1.288800</td>\n    </tr>\n    <tr>\n      <td>306</td>\n      <td>1.329900</td>\n    </tr>\n    <tr>\n      <td>307</td>\n      <td>1.512800</td>\n    </tr>\n    <tr>\n      <td>308</td>\n      <td>1.442500</td>\n    </tr>\n    <tr>\n      <td>309</td>\n      <td>1.439900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.347200</td>\n    </tr>\n    <tr>\n      <td>311</td>\n      <td>1.385400</td>\n    </tr>\n    <tr>\n      <td>312</td>\n      <td>1.644300</td>\n    </tr>\n    <tr>\n      <td>313</td>\n      <td>1.320200</td>\n    </tr>\n    <tr>\n      <td>314</td>\n      <td>1.296700</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>1.397400</td>\n    </tr>\n    <tr>\n      <td>316</td>\n      <td>1.405800</td>\n    </tr>\n    <tr>\n      <td>317</td>\n      <td>1.426500</td>\n    </tr>\n    <tr>\n      <td>318</td>\n      <td>1.486600</td>\n    </tr>\n    <tr>\n      <td>319</td>\n      <td>1.356400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.533800</td>\n    </tr>\n    <tr>\n      <td>321</td>\n      <td>1.258600</td>\n    </tr>\n    <tr>\n      <td>322</td>\n      <td>1.353000</td>\n    </tr>\n    <tr>\n      <td>323</td>\n      <td>1.196100</td>\n    </tr>\n    <tr>\n      <td>324</td>\n      <td>1.474300</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.257100</td>\n    </tr>\n    <tr>\n      <td>326</td>\n      <td>1.500700</td>\n    </tr>\n    <tr>\n      <td>327</td>\n      <td>1.490600</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>1.348100</td>\n    </tr>\n    <tr>\n      <td>329</td>\n      <td>1.485900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.363800</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>1.224800</td>\n    </tr>\n    <tr>\n      <td>332</td>\n      <td>1.282700</td>\n    </tr>\n    <tr>\n      <td>333</td>\n      <td>1.411700</td>\n    </tr>\n    <tr>\n      <td>334</td>\n      <td>1.397800</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>1.369600</td>\n    </tr>\n    <tr>\n      <td>336</td>\n      <td>1.357400</td>\n    </tr>\n    <tr>\n      <td>337</td>\n      <td>1.324100</td>\n    </tr>\n    <tr>\n      <td>338</td>\n      <td>1.313500</td>\n    </tr>\n    <tr>\n      <td>339</td>\n      <td>1.531200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.494900</td>\n    </tr>\n    <tr>\n      <td>341</td>\n      <td>1.357400</td>\n    </tr>\n    <tr>\n      <td>342</td>\n      <td>1.402200</td>\n    </tr>\n    <tr>\n      <td>343</td>\n      <td>1.424100</td>\n    </tr>\n    <tr>\n      <td>344</td>\n      <td>1.485100</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>1.289400</td>\n    </tr>\n    <tr>\n      <td>346</td>\n      <td>1.468700</td>\n    </tr>\n    <tr>\n      <td>347</td>\n      <td>1.470900</td>\n    </tr>\n    <tr>\n      <td>348</td>\n      <td>1.452800</td>\n    </tr>\n    <tr>\n      <td>349</td>\n      <td>1.318500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.463200</td>\n    </tr>\n    <tr>\n      <td>351</td>\n      <td>1.221000</td>\n    </tr>\n    <tr>\n      <td>352</td>\n      <td>1.359700</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>1.459400</td>\n    </tr>\n    <tr>\n      <td>354</td>\n      <td>1.575700</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>1.519100</td>\n    </tr>\n    <tr>\n      <td>356</td>\n      <td>1.328500</td>\n    </tr>\n    <tr>\n      <td>357</td>\n      <td>1.379800</td>\n    </tr>\n    <tr>\n      <td>358</td>\n      <td>1.238300</td>\n    </tr>\n    <tr>\n      <td>359</td>\n      <td>1.381400</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.374600</td>\n    </tr>\n    <tr>\n      <td>361</td>\n      <td>1.377600</td>\n    </tr>\n    <tr>\n      <td>362</td>\n      <td>1.417300</td>\n    </tr>\n    <tr>\n      <td>363</td>\n      <td>1.496400</td>\n    </tr>\n    <tr>\n      <td>364</td>\n      <td>1.260000</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>1.497800</td>\n    </tr>\n    <tr>\n      <td>366</td>\n      <td>1.578600</td>\n    </tr>\n    <tr>\n      <td>367</td>\n      <td>1.322300</td>\n    </tr>\n    <tr>\n      <td>368</td>\n      <td>1.405600</td>\n    </tr>\n    <tr>\n      <td>369</td>\n      <td>1.232700</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.264000</td>\n    </tr>\n    <tr>\n      <td>371</td>\n      <td>1.437600</td>\n    </tr>\n    <tr>\n      <td>372</td>\n      <td>1.435100</td>\n    </tr>\n    <tr>\n      <td>373</td>\n      <td>1.455500</td>\n    </tr>\n    <tr>\n      <td>374</td>\n      <td>1.696000</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.212800</td>\n    </tr>\n    <tr>\n      <td>376</td>\n      <td>1.579400</td>\n    </tr>\n    <tr>\n      <td>377</td>\n      <td>1.308000</td>\n    </tr>\n    <tr>\n      <td>378</td>\n      <td>1.408200</td>\n    </tr>\n    <tr>\n      <td>379</td>\n      <td>1.302000</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.500200</td>\n    </tr>\n    <tr>\n      <td>381</td>\n      <td>1.479700</td>\n    </tr>\n    <tr>\n      <td>382</td>\n      <td>1.510500</td>\n    </tr>\n    <tr>\n      <td>383</td>\n      <td>1.402800</td>\n    </tr>\n    <tr>\n      <td>384</td>\n      <td>1.492400</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>1.314800</td>\n    </tr>\n    <tr>\n      <td>386</td>\n      <td>1.217800</td>\n    </tr>\n    <tr>\n      <td>387</td>\n      <td>1.504800</td>\n    </tr>\n    <tr>\n      <td>388</td>\n      <td>1.291200</td>\n    </tr>\n    <tr>\n      <td>389</td>\n      <td>1.426600</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.177600</td>\n    </tr>\n    <tr>\n      <td>391</td>\n      <td>1.347000</td>\n    </tr>\n    <tr>\n      <td>392</td>\n      <td>1.333600</td>\n    </tr>\n    <tr>\n      <td>393</td>\n      <td>1.486100</td>\n    </tr>\n    <tr>\n      <td>394</td>\n      <td>1.432600</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>1.488900</td>\n    </tr>\n    <tr>\n      <td>396</td>\n      <td>1.289100</td>\n    </tr>\n    <tr>\n      <td>397</td>\n      <td>1.378900</td>\n    </tr>\n    <tr>\n      <td>398</td>\n      <td>1.449600</td>\n    </tr>\n    <tr>\n      <td>399</td>\n      <td>1.259000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.213300</td>\n    </tr>\n    <tr>\n      <td>401</td>\n      <td>1.451700</td>\n    </tr>\n    <tr>\n      <td>402</td>\n      <td>1.418700</td>\n    </tr>\n    <tr>\n      <td>403</td>\n      <td>1.307800</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>1.423500</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>1.454600</td>\n    </tr>\n    <tr>\n      <td>406</td>\n      <td>1.477400</td>\n    </tr>\n    <tr>\n      <td>407</td>\n      <td>1.455900</td>\n    </tr>\n    <tr>\n      <td>408</td>\n      <td>1.221000</td>\n    </tr>\n    <tr>\n      <td>409</td>\n      <td>1.236400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.493500</td>\n    </tr>\n    <tr>\n      <td>411</td>\n      <td>1.340600</td>\n    </tr>\n    <tr>\n      <td>412</td>\n      <td>1.231600</td>\n    </tr>\n    <tr>\n      <td>413</td>\n      <td>1.617800</td>\n    </tr>\n    <tr>\n      <td>414</td>\n      <td>1.350000</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>1.391600</td>\n    </tr>\n    <tr>\n      <td>416</td>\n      <td>1.417000</td>\n    </tr>\n    <tr>\n      <td>417</td>\n      <td>1.463000</td>\n    </tr>\n    <tr>\n      <td>418</td>\n      <td>1.375000</td>\n    </tr>\n    <tr>\n      <td>419</td>\n      <td>1.270600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.520100</td>\n    </tr>\n    <tr>\n      <td>421</td>\n      <td>1.371400</td>\n    </tr>\n    <tr>\n      <td>422</td>\n      <td>1.253600</td>\n    </tr>\n    <tr>\n      <td>423</td>\n      <td>1.422800</td>\n    </tr>\n    <tr>\n      <td>424</td>\n      <td>1.518800</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.171500</td>\n    </tr>\n    <tr>\n      <td>426</td>\n      <td>1.557200</td>\n    </tr>\n    <tr>\n      <td>427</td>\n      <td>1.475900</td>\n    </tr>\n    <tr>\n      <td>428</td>\n      <td>1.436800</td>\n    </tr>\n    <tr>\n      <td>429</td>\n      <td>1.584100</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.495100</td>\n    </tr>\n    <tr>\n      <td>431</td>\n      <td>1.437100</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>1.465200</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>1.392600</td>\n    </tr>\n    <tr>\n      <td>434</td>\n      <td>1.487600</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>1.337800</td>\n    </tr>\n    <tr>\n      <td>436</td>\n      <td>1.385800</td>\n    </tr>\n    <tr>\n      <td>437</td>\n      <td>1.434000</td>\n    </tr>\n    <tr>\n      <td>438</td>\n      <td>1.487500</td>\n    </tr>\n    <tr>\n      <td>439</td>\n      <td>1.395700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.270700</td>\n    </tr>\n    <tr>\n      <td>441</td>\n      <td>1.367900</td>\n    </tr>\n    <tr>\n      <td>442</td>\n      <td>1.305200</td>\n    </tr>\n    <tr>\n      <td>443</td>\n      <td>1.267000</td>\n    </tr>\n    <tr>\n      <td>444</td>\n      <td>1.371800</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>1.484100</td>\n    </tr>\n    <tr>\n      <td>446</td>\n      <td>1.358400</td>\n    </tr>\n    <tr>\n      <td>447</td>\n      <td>1.129600</td>\n    </tr>\n    <tr>\n      <td>448</td>\n      <td>1.390600</td>\n    </tr>\n    <tr>\n      <td>449</td>\n      <td>1.287200</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.328700</td>\n    </tr>\n    <tr>\n      <td>451</td>\n      <td>1.276700</td>\n    </tr>\n    <tr>\n      <td>452</td>\n      <td>1.457000</td>\n    </tr>\n    <tr>\n      <td>453</td>\n      <td>1.334300</td>\n    </tr>\n    <tr>\n      <td>454</td>\n      <td>1.283400</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>1.430900</td>\n    </tr>\n    <tr>\n      <td>456</td>\n      <td>1.466200</td>\n    </tr>\n    <tr>\n      <td>457</td>\n      <td>1.022900</td>\n    </tr>\n    <tr>\n      <td>458</td>\n      <td>1.192900</td>\n    </tr>\n    <tr>\n      <td>459</td>\n      <td>1.315300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.417500</td>\n    </tr>\n    <tr>\n      <td>461</td>\n      <td>1.365100</td>\n    </tr>\n    <tr>\n      <td>462</td>\n      <td>1.120000</td>\n    </tr>\n    <tr>\n      <td>463</td>\n      <td>1.134500</td>\n    </tr>\n    <tr>\n      <td>464</td>\n      <td>1.348300</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>1.109200</td>\n    </tr>\n    <tr>\n      <td>466</td>\n      <td>1.376800</td>\n    </tr>\n    <tr>\n      <td>467</td>\n      <td>1.207200</td>\n    </tr>\n    <tr>\n      <td>468</td>\n      <td>1.295200</td>\n    </tr>\n    <tr>\n      <td>469</td>\n      <td>1.206200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.224600</td>\n    </tr>\n    <tr>\n      <td>471</td>\n      <td>1.294900</td>\n    </tr>\n    <tr>\n      <td>472</td>\n      <td>1.037600</td>\n    </tr>\n    <tr>\n      <td>473</td>\n      <td>1.465300</td>\n    </tr>\n    <tr>\n      <td>474</td>\n      <td>1.319800</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.264700</td>\n    </tr>\n    <tr>\n      <td>476</td>\n      <td>1.117700</td>\n    </tr>\n    <tr>\n      <td>477</td>\n      <td>1.294200</td>\n    </tr>\n    <tr>\n      <td>478</td>\n      <td>1.236600</td>\n    </tr>\n    <tr>\n      <td>479</td>\n      <td>1.186200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.266000</td>\n    </tr>\n    <tr>\n      <td>481</td>\n      <td>1.141000</td>\n    </tr>\n    <tr>\n      <td>482</td>\n      <td>1.054500</td>\n    </tr>\n    <tr>\n      <td>483</td>\n      <td>1.272500</td>\n    </tr>\n    <tr>\n      <td>484</td>\n      <td>1.175700</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>1.326700</td>\n    </tr>\n    <tr>\n      <td>486</td>\n      <td>1.258900</td>\n    </tr>\n    <tr>\n      <td>487</td>\n      <td>1.321900</td>\n    </tr>\n    <tr>\n      <td>488</td>\n      <td>1.295400</td>\n    </tr>\n    <tr>\n      <td>489</td>\n      <td>1.223600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.404600</td>\n    </tr>\n    <tr>\n      <td>491</td>\n      <td>1.250700</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>1.320600</td>\n    </tr>\n    <tr>\n      <td>493</td>\n      <td>1.230300</td>\n    </tr>\n    <tr>\n      <td>494</td>\n      <td>1.301100</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>1.187000</td>\n    </tr>\n    <tr>\n      <td>496</td>\n      <td>1.452800</td>\n    </tr>\n    <tr>\n      <td>497</td>\n      <td>1.135700</td>\n    </tr>\n    <tr>\n      <td>498</td>\n      <td>1.243100</td>\n    </tr>\n    <tr>\n      <td>499</td>\n      <td>1.264300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.282100</td>\n    </tr>\n    <tr>\n      <td>501</td>\n      <td>1.255900</td>\n    </tr>\n    <tr>\n      <td>502</td>\n      <td>1.213300</td>\n    </tr>\n    <tr>\n      <td>503</td>\n      <td>1.185300</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>1.274500</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>1.242200</td>\n    </tr>\n    <tr>\n      <td>506</td>\n      <td>1.351500</td>\n    </tr>\n    <tr>\n      <td>507</td>\n      <td>1.279000</td>\n    </tr>\n    <tr>\n      <td>508</td>\n      <td>1.293000</td>\n    </tr>\n    <tr>\n      <td>509</td>\n      <td>1.179500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.194000</td>\n    </tr>\n    <tr>\n      <td>511</td>\n      <td>1.424800</td>\n    </tr>\n    <tr>\n      <td>512</td>\n      <td>1.360900</td>\n    </tr>\n    <tr>\n      <td>513</td>\n      <td>1.372700</td>\n    </tr>\n    <tr>\n      <td>514</td>\n      <td>1.387100</td>\n    </tr>\n    <tr>\n      <td>515</td>\n      <td>1.285900</td>\n    </tr>\n    <tr>\n      <td>516</td>\n      <td>1.209200</td>\n    </tr>\n    <tr>\n      <td>517</td>\n      <td>1.272000</td>\n    </tr>\n    <tr>\n      <td>518</td>\n      <td>1.085500</td>\n    </tr>\n    <tr>\n      <td>519</td>\n      <td>1.207700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.311900</td>\n    </tr>\n    <tr>\n      <td>521</td>\n      <td>1.227000</td>\n    </tr>\n    <tr>\n      <td>522</td>\n      <td>1.317600</td>\n    </tr>\n    <tr>\n      <td>523</td>\n      <td>1.317200</td>\n    </tr>\n    <tr>\n      <td>524</td>\n      <td>1.088800</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.238500</td>\n    </tr>\n    <tr>\n      <td>526</td>\n      <td>1.319900</td>\n    </tr>\n    <tr>\n      <td>527</td>\n      <td>1.325300</td>\n    </tr>\n    <tr>\n      <td>528</td>\n      <td>1.284400</td>\n    </tr>\n    <tr>\n      <td>529</td>\n      <td>1.382300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.328000</td>\n    </tr>\n    <tr>\n      <td>531</td>\n      <td>1.390600</td>\n    </tr>\n    <tr>\n      <td>532</td>\n      <td>1.463000</td>\n    </tr>\n    <tr>\n      <td>533</td>\n      <td>1.255400</td>\n    </tr>\n    <tr>\n      <td>534</td>\n      <td>1.300500</td>\n    </tr>\n    <tr>\n      <td>535</td>\n      <td>1.309800</td>\n    </tr>\n    <tr>\n      <td>536</td>\n      <td>1.338400</td>\n    </tr>\n    <tr>\n      <td>537</td>\n      <td>1.083800</td>\n    </tr>\n    <tr>\n      <td>538</td>\n      <td>1.291600</td>\n    </tr>\n    <tr>\n      <td>539</td>\n      <td>1.260000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.326100</td>\n    </tr>\n    <tr>\n      <td>541</td>\n      <td>1.170200</td>\n    </tr>\n    <tr>\n      <td>542</td>\n      <td>1.218800</td>\n    </tr>\n    <tr>\n      <td>543</td>\n      <td>1.055500</td>\n    </tr>\n    <tr>\n      <td>544</td>\n      <td>1.210000</td>\n    </tr>\n    <tr>\n      <td>545</td>\n      <td>1.389400</td>\n    </tr>\n    <tr>\n      <td>546</td>\n      <td>1.276700</td>\n    </tr>\n    <tr>\n      <td>547</td>\n      <td>1.208500</td>\n    </tr>\n    <tr>\n      <td>548</td>\n      <td>1.221000</td>\n    </tr>\n    <tr>\n      <td>549</td>\n      <td>1.331400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.301300</td>\n    </tr>\n    <tr>\n      <td>551</td>\n      <td>1.323500</td>\n    </tr>\n    <tr>\n      <td>552</td>\n      <td>1.321600</td>\n    </tr>\n    <tr>\n      <td>553</td>\n      <td>1.097400</td>\n    </tr>\n    <tr>\n      <td>554</td>\n      <td>1.443100</td>\n    </tr>\n    <tr>\n      <td>555</td>\n      <td>1.151800</td>\n    </tr>\n    <tr>\n      <td>556</td>\n      <td>1.323600</td>\n    </tr>\n    <tr>\n      <td>557</td>\n      <td>1.235700</td>\n    </tr>\n    <tr>\n      <td>558</td>\n      <td>1.325100</td>\n    </tr>\n    <tr>\n      <td>559</td>\n      <td>1.124700</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.485800</td>\n    </tr>\n    <tr>\n      <td>561</td>\n      <td>1.136700</td>\n    </tr>\n    <tr>\n      <td>562</td>\n      <td>1.196100</td>\n    </tr>\n    <tr>\n      <td>563</td>\n      <td>1.189400</td>\n    </tr>\n    <tr>\n      <td>564</td>\n      <td>0.989800</td>\n    </tr>\n    <tr>\n      <td>565</td>\n      <td>1.358800</td>\n    </tr>\n    <tr>\n      <td>566</td>\n      <td>1.109000</td>\n    </tr>\n    <tr>\n      <td>567</td>\n      <td>1.274500</td>\n    </tr>\n    <tr>\n      <td>568</td>\n      <td>1.288200</td>\n    </tr>\n    <tr>\n      <td>569</td>\n      <td>1.304200</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.186400</td>\n    </tr>\n    <tr>\n      <td>571</td>\n      <td>1.351400</td>\n    </tr>\n    <tr>\n      <td>572</td>\n      <td>1.232900</td>\n    </tr>\n    <tr>\n      <td>573</td>\n      <td>1.291300</td>\n    </tr>\n    <tr>\n      <td>574</td>\n      <td>1.207300</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.266200</td>\n    </tr>\n    <tr>\n      <td>576</td>\n      <td>1.115400</td>\n    </tr>\n    <tr>\n      <td>577</td>\n      <td>1.256100</td>\n    </tr>\n    <tr>\n      <td>578</td>\n      <td>1.403400</td>\n    </tr>\n    <tr>\n      <td>579</td>\n      <td>1.327400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.259500</td>\n    </tr>\n    <tr>\n      <td>581</td>\n      <td>1.246100</td>\n    </tr>\n    <tr>\n      <td>582</td>\n      <td>0.998900</td>\n    </tr>\n    <tr>\n      <td>583</td>\n      <td>1.255800</td>\n    </tr>\n    <tr>\n      <td>584</td>\n      <td>1.350600</td>\n    </tr>\n    <tr>\n      <td>585</td>\n      <td>1.162300</td>\n    </tr>\n    <tr>\n      <td>586</td>\n      <td>1.107200</td>\n    </tr>\n    <tr>\n      <td>587</td>\n      <td>1.311100</td>\n    </tr>\n    <tr>\n      <td>588</td>\n      <td>1.093300</td>\n    </tr>\n    <tr>\n      <td>589</td>\n      <td>1.220200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.268000</td>\n    </tr>\n    <tr>\n      <td>591</td>\n      <td>1.222400</td>\n    </tr>\n    <tr>\n      <td>592</td>\n      <td>1.352200</td>\n    </tr>\n    <tr>\n      <td>593</td>\n      <td>1.009200</td>\n    </tr>\n    <tr>\n      <td>594</td>\n      <td>1.187000</td>\n    </tr>\n    <tr>\n      <td>595</td>\n      <td>1.350700</td>\n    </tr>\n    <tr>\n      <td>596</td>\n      <td>1.341000</td>\n    </tr>\n    <tr>\n      <td>597</td>\n      <td>1.275400</td>\n    </tr>\n    <tr>\n      <td>598</td>\n      <td>1.355600</td>\n    </tr>\n    <tr>\n      <td>599</td>\n      <td>1.287100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.196300</td>\n    </tr>\n    <tr>\n      <td>601</td>\n      <td>1.309700</td>\n    </tr>\n    <tr>\n      <td>602</td>\n      <td>1.145300</td>\n    </tr>\n    <tr>\n      <td>603</td>\n      <td>1.310400</td>\n    </tr>\n    <tr>\n      <td>604</td>\n      <td>1.356000</td>\n    </tr>\n    <tr>\n      <td>605</td>\n      <td>1.385700</td>\n    </tr>\n    <tr>\n      <td>606</td>\n      <td>1.054300</td>\n    </tr>\n    <tr>\n      <td>607</td>\n      <td>1.305000</td>\n    </tr>\n    <tr>\n      <td>608</td>\n      <td>1.113300</td>\n    </tr>\n    <tr>\n      <td>609</td>\n      <td>1.459200</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.384800</td>\n    </tr>\n    <tr>\n      <td>611</td>\n      <td>1.211100</td>\n    </tr>\n    <tr>\n      <td>612</td>\n      <td>1.186800</td>\n    </tr>\n    <tr>\n      <td>613</td>\n      <td>1.177300</td>\n    </tr>\n    <tr>\n      <td>614</td>\n      <td>1.229800</td>\n    </tr>\n    <tr>\n      <td>615</td>\n      <td>1.310200</td>\n    </tr>\n    <tr>\n      <td>616</td>\n      <td>1.287200</td>\n    </tr>\n    <tr>\n      <td>617</td>\n      <td>1.175400</td>\n    </tr>\n    <tr>\n      <td>618</td>\n      <td>1.339000</td>\n    </tr>\n    <tr>\n      <td>619</td>\n      <td>1.164200</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.308000</td>\n    </tr>\n    <tr>\n      <td>621</td>\n      <td>1.114100</td>\n    </tr>\n    <tr>\n      <td>622</td>\n      <td>1.107000</td>\n    </tr>\n    <tr>\n      <td>623</td>\n      <td>1.236000</td>\n    </tr>\n    <tr>\n      <td>624</td>\n      <td>1.183600</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.162100</td>\n    </tr>\n    <tr>\n      <td>626</td>\n      <td>1.409600</td>\n    </tr>\n    <tr>\n      <td>627</td>\n      <td>1.145700</td>\n    </tr>\n    <tr>\n      <td>628</td>\n      <td>1.263600</td>\n    </tr>\n    <tr>\n      <td>629</td>\n      <td>1.384800</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.283500</td>\n    </tr>\n    <tr>\n      <td>631</td>\n      <td>1.194500</td>\n    </tr>\n    <tr>\n      <td>632</td>\n      <td>1.241000</td>\n    </tr>\n    <tr>\n      <td>633</td>\n      <td>1.229000</td>\n    </tr>\n    <tr>\n      <td>634</td>\n      <td>1.391900</td>\n    </tr>\n    <tr>\n      <td>635</td>\n      <td>1.204900</td>\n    </tr>\n    <tr>\n      <td>636</td>\n      <td>1.365200</td>\n    </tr>\n    <tr>\n      <td>637</td>\n      <td>1.307900</td>\n    </tr>\n    <tr>\n      <td>638</td>\n      <td>1.314100</td>\n    </tr>\n    <tr>\n      <td>639</td>\n      <td>1.202000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.232700</td>\n    </tr>\n    <tr>\n      <td>641</td>\n      <td>1.364600</td>\n    </tr>\n    <tr>\n      <td>642</td>\n      <td>1.354000</td>\n    </tr>\n    <tr>\n      <td>643</td>\n      <td>1.245200</td>\n    </tr>\n    <tr>\n      <td>644</td>\n      <td>1.258500</td>\n    </tr>\n    <tr>\n      <td>645</td>\n      <td>1.265100</td>\n    </tr>\n    <tr>\n      <td>646</td>\n      <td>1.341400</td>\n    </tr>\n    <tr>\n      <td>647</td>\n      <td>1.324600</td>\n    </tr>\n    <tr>\n      <td>648</td>\n      <td>1.327100</td>\n    </tr>\n    <tr>\n      <td>649</td>\n      <td>1.331500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.294800</td>\n    </tr>\n    <tr>\n      <td>651</td>\n      <td>1.481900</td>\n    </tr>\n    <tr>\n      <td>652</td>\n      <td>1.253600</td>\n    </tr>\n    <tr>\n      <td>653</td>\n      <td>1.147800</td>\n    </tr>\n    <tr>\n      <td>654</td>\n      <td>1.137400</td>\n    </tr>\n    <tr>\n      <td>655</td>\n      <td>1.041900</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>1.339200</td>\n    </tr>\n    <tr>\n      <td>657</td>\n      <td>1.167900</td>\n    </tr>\n    <tr>\n      <td>658</td>\n      <td>1.232400</td>\n    </tr>\n    <tr>\n      <td>659</td>\n      <td>1.355200</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.227500</td>\n    </tr>\n    <tr>\n      <td>661</td>\n      <td>0.985200</td>\n    </tr>\n    <tr>\n      <td>662</td>\n      <td>1.241600</td>\n    </tr>\n    <tr>\n      <td>663</td>\n      <td>1.337400</td>\n    </tr>\n    <tr>\n      <td>664</td>\n      <td>1.218100</td>\n    </tr>\n    <tr>\n      <td>665</td>\n      <td>1.216300</td>\n    </tr>\n    <tr>\n      <td>666</td>\n      <td>1.329600</td>\n    </tr>\n    <tr>\n      <td>667</td>\n      <td>1.360200</td>\n    </tr>\n    <tr>\n      <td>668</td>\n      <td>1.457300</td>\n    </tr>\n    <tr>\n      <td>669</td>\n      <td>1.324100</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.324900</td>\n    </tr>\n    <tr>\n      <td>671</td>\n      <td>1.096100</td>\n    </tr>\n    <tr>\n      <td>672</td>\n      <td>1.246400</td>\n    </tr>\n    <tr>\n      <td>673</td>\n      <td>1.227900</td>\n    </tr>\n    <tr>\n      <td>674</td>\n      <td>1.293700</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.176200</td>\n    </tr>\n    <tr>\n      <td>676</td>\n      <td>1.197400</td>\n    </tr>\n    <tr>\n      <td>677</td>\n      <td>1.159700</td>\n    </tr>\n    <tr>\n      <td>678</td>\n      <td>1.083600</td>\n    </tr>\n    <tr>\n      <td>679</td>\n      <td>1.340600</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.176300</td>\n    </tr>\n    <tr>\n      <td>681</td>\n      <td>1.463100</td>\n    </tr>\n    <tr>\n      <td>682</td>\n      <td>1.241000</td>\n    </tr>\n    <tr>\n      <td>683</td>\n      <td>1.352600</td>\n    </tr>\n    <tr>\n      <td>684</td>\n      <td>1.297100</td>\n    </tr>\n    <tr>\n      <td>685</td>\n      <td>1.159500</td>\n    </tr>\n    <tr>\n      <td>686</td>\n      <td>1.280800</td>\n    </tr>\n    <tr>\n      <td>687</td>\n      <td>1.097900</td>\n    </tr>\n    <tr>\n      <td>688</td>\n      <td>1.318500</td>\n    </tr>\n    <tr>\n      <td>689</td>\n      <td>1.141800</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.449900</td>\n    </tr>\n    <tr>\n      <td>691</td>\n      <td>1.239200</td>\n    </tr>\n    <tr>\n      <td>692</td>\n      <td>1.258600</td>\n    </tr>\n    <tr>\n      <td>693</td>\n      <td>1.371700</td>\n    </tr>\n    <tr>\n      <td>694</td>\n      <td>1.258400</td>\n    </tr>\n    <tr>\n      <td>695</td>\n      <td>1.255000</td>\n    </tr>\n    <tr>\n      <td>696</td>\n      <td>1.363500</td>\n    </tr>\n    <tr>\n      <td>697</td>\n      <td>1.225100</td>\n    </tr>\n    <tr>\n      <td>698</td>\n      <td>1.385200</td>\n    </tr>\n    <tr>\n      <td>699</td>\n      <td>1.197200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.171600</td>\n    </tr>\n    <tr>\n      <td>701</td>\n      <td>1.349500</td>\n    </tr>\n    <tr>\n      <td>702</td>\n      <td>1.100600</td>\n    </tr>\n    <tr>\n      <td>703</td>\n      <td>1.309200</td>\n    </tr>\n    <tr>\n      <td>704</td>\n      <td>1.219000</td>\n    </tr>\n    <tr>\n      <td>705</td>\n      <td>1.237100</td>\n    </tr>\n    <tr>\n      <td>706</td>\n      <td>1.226900</td>\n    </tr>\n    <tr>\n      <td>707</td>\n      <td>1.153900</td>\n    </tr>\n    <tr>\n      <td>708</td>\n      <td>1.480800</td>\n    </tr>\n    <tr>\n      <td>709</td>\n      <td>1.216000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.213200</td>\n    </tr>\n    <tr>\n      <td>711</td>\n      <td>1.146700</td>\n    </tr>\n    <tr>\n      <td>712</td>\n      <td>1.403200</td>\n    </tr>\n    <tr>\n      <td>713</td>\n      <td>1.333600</td>\n    </tr>\n    <tr>\n      <td>714</td>\n      <td>1.279000</td>\n    </tr>\n    <tr>\n      <td>715</td>\n      <td>1.311200</td>\n    </tr>\n    <tr>\n      <td>716</td>\n      <td>1.238700</td>\n    </tr>\n    <tr>\n      <td>717</td>\n      <td>1.243400</td>\n    </tr>\n    <tr>\n      <td>718</td>\n      <td>1.255000</td>\n    </tr>\n    <tr>\n      <td>719</td>\n      <td>1.253100</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.372400</td>\n    </tr>\n    <tr>\n      <td>721</td>\n      <td>1.160600</td>\n    </tr>\n    <tr>\n      <td>722</td>\n      <td>1.240300</td>\n    </tr>\n    <tr>\n      <td>723</td>\n      <td>1.084600</td>\n    </tr>\n    <tr>\n      <td>724</td>\n      <td>1.250900</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.219600</td>\n    </tr>\n    <tr>\n      <td>726</td>\n      <td>1.326600</td>\n    </tr>\n    <tr>\n      <td>727</td>\n      <td>1.253800</td>\n    </tr>\n    <tr>\n      <td>728</td>\n      <td>1.209200</td>\n    </tr>\n    <tr>\n      <td>729</td>\n      <td>1.306000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.334700</td>\n    </tr>\n    <tr>\n      <td>731</td>\n      <td>1.240300</td>\n    </tr>\n    <tr>\n      <td>732</td>\n      <td>1.203100</td>\n    </tr>\n    <tr>\n      <td>733</td>\n      <td>1.238100</td>\n    </tr>\n    <tr>\n      <td>734</td>\n      <td>1.219600</td>\n    </tr>\n    <tr>\n      <td>735</td>\n      <td>1.309100</td>\n    </tr>\n    <tr>\n      <td>736</td>\n      <td>1.335800</td>\n    </tr>\n    <tr>\n      <td>737</td>\n      <td>1.132800</td>\n    </tr>\n    <tr>\n      <td>738</td>\n      <td>1.334000</td>\n    </tr>\n    <tr>\n      <td>739</td>\n      <td>1.243500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.161100</td>\n    </tr>\n    <tr>\n      <td>741</td>\n      <td>1.194500</td>\n    </tr>\n    <tr>\n      <td>742</td>\n      <td>1.391700</td>\n    </tr>\n    <tr>\n      <td>743</td>\n      <td>1.275600</td>\n    </tr>\n    <tr>\n      <td>744</td>\n      <td>1.282100</td>\n    </tr>\n    <tr>\n      <td>745</td>\n      <td>1.216400</td>\n    </tr>\n    <tr>\n      <td>746</td>\n      <td>1.466900</td>\n    </tr>\n    <tr>\n      <td>747</td>\n      <td>1.299800</td>\n    </tr>\n    <tr>\n      <td>748</td>\n      <td>1.383500</td>\n    </tr>\n    <tr>\n      <td>749</td>\n      <td>1.237300</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.295700</td>\n    </tr>\n    <tr>\n      <td>751</td>\n      <td>1.401300</td>\n    </tr>\n    <tr>\n      <td>752</td>\n      <td>1.108600</td>\n    </tr>\n    <tr>\n      <td>753</td>\n      <td>1.301900</td>\n    </tr>\n    <tr>\n      <td>754</td>\n      <td>1.244300</td>\n    </tr>\n    <tr>\n      <td>755</td>\n      <td>1.322400</td>\n    </tr>\n    <tr>\n      <td>756</td>\n      <td>1.338800</td>\n    </tr>\n    <tr>\n      <td>757</td>\n      <td>1.495300</td>\n    </tr>\n    <tr>\n      <td>758</td>\n      <td>1.412700</td>\n    </tr>\n    <tr>\n      <td>759</td>\n      <td>1.290000</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.456100</td>\n    </tr>\n    <tr>\n      <td>761</td>\n      <td>1.214800</td>\n    </tr>\n    <tr>\n      <td>762</td>\n      <td>1.305000</td>\n    </tr>\n    <tr>\n      <td>763</td>\n      <td>1.169800</td>\n    </tr>\n    <tr>\n      <td>764</td>\n      <td>1.262600</td>\n    </tr>\n    <tr>\n      <td>765</td>\n      <td>0.962400</td>\n    </tr>\n    <tr>\n      <td>766</td>\n      <td>1.429400</td>\n    </tr>\n    <tr>\n      <td>767</td>\n      <td>1.022800</td>\n    </tr>\n    <tr>\n      <td>768</td>\n      <td>1.380600</td>\n    </tr>\n    <tr>\n      <td>769</td>\n      <td>1.249700</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.260900</td>\n    </tr>\n    <tr>\n      <td>771</td>\n      <td>1.271300</td>\n    </tr>\n    <tr>\n      <td>772</td>\n      <td>1.324100</td>\n    </tr>\n    <tr>\n      <td>773</td>\n      <td>1.137000</td>\n    </tr>\n    <tr>\n      <td>774</td>\n      <td>1.257700</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.239800</td>\n    </tr>\n    <tr>\n      <td>776</td>\n      <td>1.272900</td>\n    </tr>\n    <tr>\n      <td>777</td>\n      <td>1.221700</td>\n    </tr>\n    <tr>\n      <td>778</td>\n      <td>1.223700</td>\n    </tr>\n    <tr>\n      <td>779</td>\n      <td>1.397400</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.272000</td>\n    </tr>\n    <tr>\n      <td>781</td>\n      <td>1.320800</td>\n    </tr>\n    <tr>\n      <td>782</td>\n      <td>1.340800</td>\n    </tr>\n    <tr>\n      <td>783</td>\n      <td>1.166200</td>\n    </tr>\n    <tr>\n      <td>784</td>\n      <td>1.181000</td>\n    </tr>\n    <tr>\n      <td>785</td>\n      <td>1.295400</td>\n    </tr>\n    <tr>\n      <td>786</td>\n      <td>1.295800</td>\n    </tr>\n    <tr>\n      <td>787</td>\n      <td>1.301200</td>\n    </tr>\n    <tr>\n      <td>788</td>\n      <td>1.497700</td>\n    </tr>\n    <tr>\n      <td>789</td>\n      <td>1.236800</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.231200</td>\n    </tr>\n    <tr>\n      <td>791</td>\n      <td>1.372000</td>\n    </tr>\n    <tr>\n      <td>792</td>\n      <td>1.201100</td>\n    </tr>\n    <tr>\n      <td>793</td>\n      <td>1.284800</td>\n    </tr>\n    <tr>\n      <td>794</td>\n      <td>1.279400</td>\n    </tr>\n    <tr>\n      <td>795</td>\n      <td>1.060700</td>\n    </tr>\n    <tr>\n      <td>796</td>\n      <td>1.279300</td>\n    </tr>\n    <tr>\n      <td>797</td>\n      <td>1.154900</td>\n    </tr>\n    <tr>\n      <td>798</td>\n      <td>1.384600</td>\n    </tr>\n    <tr>\n      <td>799</td>\n      <td>1.186200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.295900</td>\n    </tr>\n    <tr>\n      <td>801</td>\n      <td>1.172100</td>\n    </tr>\n    <tr>\n      <td>802</td>\n      <td>1.297000</td>\n    </tr>\n    <tr>\n      <td>803</td>\n      <td>1.219600</td>\n    </tr>\n    <tr>\n      <td>804</td>\n      <td>1.324100</td>\n    </tr>\n    <tr>\n      <td>805</td>\n      <td>1.210900</td>\n    </tr>\n    <tr>\n      <td>806</td>\n      <td>1.290000</td>\n    </tr>\n    <tr>\n      <td>807</td>\n      <td>1.176800</td>\n    </tr>\n    <tr>\n      <td>808</td>\n      <td>1.340900</td>\n    </tr>\n    <tr>\n      <td>809</td>\n      <td>1.188900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.319900</td>\n    </tr>\n    <tr>\n      <td>811</td>\n      <td>1.007600</td>\n    </tr>\n    <tr>\n      <td>812</td>\n      <td>1.334600</td>\n    </tr>\n    <tr>\n      <td>813</td>\n      <td>1.212500</td>\n    </tr>\n    <tr>\n      <td>814</td>\n      <td>1.272700</td>\n    </tr>\n    <tr>\n      <td>815</td>\n      <td>1.336800</td>\n    </tr>\n    <tr>\n      <td>816</td>\n      <td>1.056200</td>\n    </tr>\n    <tr>\n      <td>817</td>\n      <td>1.295400</td>\n    </tr>\n    <tr>\n      <td>818</td>\n      <td>1.394600</td>\n    </tr>\n    <tr>\n      <td>819</td>\n      <td>1.250800</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.270300</td>\n    </tr>\n    <tr>\n      <td>821</td>\n      <td>1.204100</td>\n    </tr>\n    <tr>\n      <td>822</td>\n      <td>1.191600</td>\n    </tr>\n    <tr>\n      <td>823</td>\n      <td>1.010800</td>\n    </tr>\n    <tr>\n      <td>824</td>\n      <td>1.179400</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>1.323500</td>\n    </tr>\n    <tr>\n      <td>826</td>\n      <td>1.234100</td>\n    </tr>\n    <tr>\n      <td>827</td>\n      <td>1.424200</td>\n    </tr>\n    <tr>\n      <td>828</td>\n      <td>1.349200</td>\n    </tr>\n    <tr>\n      <td>829</td>\n      <td>1.305100</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.356900</td>\n    </tr>\n    <tr>\n      <td>831</td>\n      <td>1.099500</td>\n    </tr>\n    <tr>\n      <td>832</td>\n      <td>1.323500</td>\n    </tr>\n    <tr>\n      <td>833</td>\n      <td>1.268100</td>\n    </tr>\n    <tr>\n      <td>834</td>\n      <td>1.342700</td>\n    </tr>\n    <tr>\n      <td>835</td>\n      <td>1.312600</td>\n    </tr>\n    <tr>\n      <td>836</td>\n      <td>1.372100</td>\n    </tr>\n    <tr>\n      <td>837</td>\n      <td>1.395100</td>\n    </tr>\n    <tr>\n      <td>838</td>\n      <td>1.309000</td>\n    </tr>\n    <tr>\n      <td>839</td>\n      <td>1.255600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.348500</td>\n    </tr>\n    <tr>\n      <td>841</td>\n      <td>1.325900</td>\n    </tr>\n    <tr>\n      <td>842</td>\n      <td>1.413700</td>\n    </tr>\n    <tr>\n      <td>843</td>\n      <td>1.363100</td>\n    </tr>\n    <tr>\n      <td>844</td>\n      <td>1.105700</td>\n    </tr>\n    <tr>\n      <td>845</td>\n      <td>1.376700</td>\n    </tr>\n    <tr>\n      <td>846</td>\n      <td>1.287300</td>\n    </tr>\n    <tr>\n      <td>847</td>\n      <td>1.379500</td>\n    </tr>\n    <tr>\n      <td>848</td>\n      <td>1.361900</td>\n    </tr>\n    <tr>\n      <td>849</td>\n      <td>1.247900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.207100</td>\n    </tr>\n    <tr>\n      <td>851</td>\n      <td>1.326600</td>\n    </tr>\n    <tr>\n      <td>852</td>\n      <td>1.164300</td>\n    </tr>\n    <tr>\n      <td>853</td>\n      <td>1.349400</td>\n    </tr>\n    <tr>\n      <td>854</td>\n      <td>1.174700</td>\n    </tr>\n    <tr>\n      <td>855</td>\n      <td>1.158100</td>\n    </tr>\n    <tr>\n      <td>856</td>\n      <td>1.311500</td>\n    </tr>\n    <tr>\n      <td>857</td>\n      <td>1.315300</td>\n    </tr>\n    <tr>\n      <td>858</td>\n      <td>1.194700</td>\n    </tr>\n    <tr>\n      <td>859</td>\n      <td>1.283200</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.247200</td>\n    </tr>\n    <tr>\n      <td>861</td>\n      <td>1.220400</td>\n    </tr>\n    <tr>\n      <td>862</td>\n      <td>1.381200</td>\n    </tr>\n    <tr>\n      <td>863</td>\n      <td>1.172000</td>\n    </tr>\n    <tr>\n      <td>864</td>\n      <td>1.169700</td>\n    </tr>\n    <tr>\n      <td>865</td>\n      <td>1.241500</td>\n    </tr>\n    <tr>\n      <td>866</td>\n      <td>1.294400</td>\n    </tr>\n    <tr>\n      <td>867</td>\n      <td>1.031600</td>\n    </tr>\n    <tr>\n      <td>868</td>\n      <td>1.367300</td>\n    </tr>\n    <tr>\n      <td>869</td>\n      <td>1.215900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.191100</td>\n    </tr>\n    <tr>\n      <td>871</td>\n      <td>1.146100</td>\n    </tr>\n    <tr>\n      <td>872</td>\n      <td>1.186200</td>\n    </tr>\n    <tr>\n      <td>873</td>\n      <td>1.299000</td>\n    </tr>\n    <tr>\n      <td>874</td>\n      <td>1.352500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"## Save Model","metadata":{}},{"cell_type":"code","source":"finetuned_model = '/kaggle/working/Deepseek-r1-finetuned_health-8B'\nmodel.save_pretrained(finetuned_model)\ntokenizer.save_pretrained(finetuned_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T06:34:48.536934Z","iopub.execute_input":"2025-02-25T06:34:48.537262Z","iopub.status.idle":"2025-02-25T06:34:49.515082Z","shell.execute_reply.started":"2025-02-25T06:34:48.537228Z","shell.execute_reply":"2025-02-25T06:34:49.514151Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/Deepseek-r1-finetuned_health-8B/tokenizer_config.json',\n '/kaggle/working/Deepseek-r1-finetuned_health-8B/special_tokens_map.json',\n '/kaggle/working/Deepseek-r1-finetuned_health-8B/tokenizer.json')"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Check the fine-tuned model on the same,old question","metadata":{}},{"cell_type":"code","source":"# Test the finetuned model now with the same input\ntokenizer=get_chat_template(\n    tokenizer,\n    chat_template='llama-3.2',\n)\n\n#Set the PAD to be the same as the EOS token and avoid tokenization issues\ntokenizer.pad_token=tokenizer.eos_token\nFastLanguageModel.for_inference(model) #enable native 2x faster inference\n\nmessages=[\n    {'role':'user','content':'I have a headache, a bad one around the forehead'}\n]\n#Tokenize the user input with the chat template\n\ninputs=tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors='pt',\n    padding=True,# Add padding to match sequence length\n\n).to('cuda')\n\nattention_mask=inputs!=tokenizer.pad_token_id\n#Generate the output\noutputs=model.generate(\n    input_ids=inputs,\n    attention_mask=attention_mask,\n    max_new_tokens=64,\n    use_cache=True, #use cache for faster token generation\n    temperature=0.6, # controls randomness in response\n    min_p=0.1 #Sets the minimum probability threshold for token selection\n)\n\n#Decode the generated tokens into human-readable text\ntext=tokenizer.decode(outputs[0],skip_special_tokens=True)\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:35:53.618424Z","iopub.execute_input":"2025-02-25T07:35:53.618822Z","iopub.status.idle":"2025-02-25T07:35:57.765536Z","shell.execute_reply.started":"2025-02-25T07:35:53.618790Z","shell.execute_reply":"2025-02-25T07:35:57.764762Z"}},"outputs":[{"name":"stdout","text":"Model does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\nsystem\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\nuser\n\nI have a headache, a bad one around the foreheadassistant\n\nOkay, so I have this really bad headache, and it's specifically around the forehead. That's kind of unusual, isn't it? Most headaches I've had before seem to be more around the whole head, you know, like around the eyes or the back of the head. This one seems to be more localized\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"### The model gives a better answer to the prompt about headache. ","metadata":{}},{"cell_type":"markdown","source":"## User interaction via Terminal\n\nThis section will querry the model on live prompts","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom nltk.translate.bleu_score import sentence_bleu\n\n# Load the fine-tuned model and tokenizer from the saved directory\nfinetuned_model_path = \"/kaggle/input/model_folder/pytorch/default/1/kaggle/working/Deepseek-r1-finetuned_health-8B\"\n\n# Load tokenizer and model from local directory\ntokenizer = AutoTokenizer.from_pretrained(finetuned_model_path, local_files_only=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(finetuned_model_path, local_files_only=False, trust_remote_code=True).to(\"cuda\")\n\n# Function to calculate Perplexity\ndef calculate_perplexity(model, tokenizer, text, device='cuda'):\n    model.eval()  # Set model to evaluation mode\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    with torch.no_grad():  \n        outputs = model(**inputs)\n        loss = F.cross_entropy(outputs.logits.view(-1, outputs.logits.size(-1)), inputs.input_ids.view(-1), ignore_index=tokenizer.pad_token_id)\n    \n    perplexity = torch.exp(loss)  # e^(cross-entropy loss)\n    return perplexity.item()\n\n# Function to calculate BLEU Score\ndef calculate_bleu(reference, generated):\n    reference = [reference.split()]  # BLEU expects list of lists\n    generated = generated.split()\n    bleu_score = sentence_bleu(reference, generated)\n    return bleu_score\n\n# Set PAD token to be the same as EOS token to avoid tokenization issues\ntokenizer.pad_token = tokenizer.eos_token\n\nwhile True:\n    # Get user input\n    user_input = input(\"Enter your message (or type 'quit' to exit): \")\n\n    # Break the loop if the user types 'quit'\n    if user_input.lower() == \"quit\":\n        print(\"Exiting...\")\n        break\n\n    # Tokenize the user input\n    inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n    attention_mask = inputs[\"input_ids\"] != tokenizer.pad_token_id\n\n    # Generate the output\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=attention_mask,\n        max_new_tokens=64,\n        use_cache=True,  # use cache for faster token generation\n        temperature=0.6,  # controls randomness in response\n        min_p=0.1  # Sets the minimum probability threshold for token selection\n    )\n\n    # Decode the generated tokens into human-readable text\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(\"Model Response:\", text)\n\n    # Calculate Perplexity\n    perplexity = calculate_perplexity(model, tokenizer, text)\n    print(f\"Perplexity Score: {perplexity}\")\n\n    # Calculate BLEU Score\n    bleu_score = calculate_bleu(user_input, text)\n    print(f\"BLEU Score: {bleu_score}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:55:23.354489Z","iopub.execute_input":"2025-02-25T08:55:23.354831Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7d66b9feeac416ebb6c500139657a9d"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a4768e2fa084dc59ebbd2a6e52ebfe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5da599b3dd4588bca5fb02ebaf823a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter your message (or type 'quit' to exit):  Hello\n"},{"name":"stdout","text":"Model Response: Hello, I'm trying to figure out how to make a simple to-do list app. I know I need to use some programming language, but I'm not sure which one to choose. I've heard of Python and JavaScript are pretty popular for this kind of thing. Let me think about which one would be easier for me\nPerplexity Score: inf\nBLEU Score: 0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message (or type 'quit' to exit):  I have a headache\n"},{"name":"stdout","text":"Model Response: I have a headache. I think I need to take some medication. Let me see what I have in the medicine cabinet.\n\nOkay, let's see... There's a bottle of Tylenol. That's good because it doesn't have any aspirin. I should be fine taking it.\n\nHmm, there's also a bottle of Adv\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Perplexity Score: inf\nBLEU Score: 0.08324415467017712\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message (or type 'quit' to exit):  I have a stomach pain\n"},{"name":"stdout","text":"Model Response: I have a stomach pain and it's been bothering me for a few days now. I need to figure out what's going on. Let's think about what could be causing this. I remember that sometimes eating too much or not enough, or maybe something I ate was a little off, could lead to this kind of discomfort. Hmm, let\nPerplexity Score: inf\nBLEU Score: 0.05757177103786432\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your message (or type 'quit' to exit):  I have a kidney stone\n"},{"name":"stdout","text":"Model Response: I have a kidney stone, and I'm not sure how to help myself with it. I know it's something that can cause a lot of pain, and I'm worried about what to do if it gets really bad. Let me think about what I can do to manage this.\n\nFirst, I should probably stay hydrated. I've heard that\nPerplexity Score: inf\nBLEU Score: 0.03918225430439208\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\n\nfinetuned_model_path = \"/kaggle/input/model_folder/pytorch/default/1/kaggle/working/Deepseek-r1-finetuned_health-8B\"\n\n# List all files in the directory\nos.listdir(finetuned_model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:49:33.619021Z","iopub.execute_input":"2025-02-25T08:49:33.619404Z","iopub.status.idle":"2025-02-25T08:49:33.626737Z","shell.execute_reply.started":"2025-02-25T08:49:33.619377Z","shell.execute_reply":"2025-02-25T08:49:33.625935Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['adapter_model.safetensors',\n 'adapter_config.json',\n 'README.md',\n 'tokenizer.json',\n 'tokenizer_config.json',\n 'special_tokens_map.json']"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## This section sets the Streamlit version of the app.\n\nIt gets too heavy to run because I use the free version of Notebook with less RAM\n","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nfrom unsloth.chat_templates import get_chat_template\n\nimport torch\n\n# Set up Streamlit UI\nst.title(\"Chat with Finetuned Model\")\n\n# Initialize tokenizer and model (ensure they're properly loaded)\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template='llama-3.2',\n)\n\n# Set the PAD to be the same as the EOS token and avoid tokenization issues\ntokenizer.pad_token = tokenizer.eos_token\nFastLanguageModel.for_inference(model)  # enable native 2x faster inference\n\n# Streamlit input box\nuser_input = st.text_area(\"Enter your message:\", \"\")\n\nif st.button(\"Generate Response\"):\n    if user_input.strip():\n        messages = [{'role': 'user', 'content': user_input}]\n\n        # Tokenize the user input with the chat template\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors='pt',\n            padding=True,  # Add padding to match sequence length\n        ).to('cuda')\n\n        attention_mask = inputs != tokenizer.pad_token_id\n\n        # Generate the output\n        outputs = model.generate(\n            input_ids=inputs,\n            attention_mask=attention_mask,\n            max_new_tokens=64,\n            use_cache=True,  # use cache for faster token generation\n            temperature=0.6,  # controls randomness in response\n            min_p=0.1  # Sets the minimum probability threshold for token selection\n        )\n\n        # Decode and display the full generated text\n        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        st.subheader(\"Model Response:\")\n        st.write(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:36:29.939581Z","iopub.execute_input":"2025-02-25T08:36:29.939905Z","iopub.status.idle":"2025-02-25T08:36:29.945206Z","shell.execute_reply.started":"2025-02-25T08:36:29.939874Z","shell.execute_reply":"2025-02-25T08:36:29.944402Z"}},"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!wget -q -O - ipv4.icanhazip.com","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:56:13.712744Z","iopub.execute_input":"2025-02-25T07:56:13.713089Z","iopub.status.idle":"2025-02-25T07:56:13.956484Z","shell.execute_reply.started":"2025-02-25T07:56:13.713065Z","shell.execute_reply":"2025-02-25T07:56:13.955472Z"}},"outputs":[{"name":"stdout","text":"34.105.33.39\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"! streamlit run app.py & npx localtunnel --port 8501\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:59:29.513259Z","iopub.execute_input":"2025-02-25T07:59:29.513565Z","execution_failed":"2025-02-25T08:20:37.420Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.105.33.39:8501\u001b[0m\n\u001b[0m\nyour url is: https://slow-mammals-repeat.loca.lt\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-02-25 07:59:58.199530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-25 07:59:58.225414: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-25 07:59:58.234098: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n🦥 Unsloth Zoo will now patch everything to make training faster!\n2025-02-25 08:00:04.933 Uncaught app execution\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 121, in exec_func_with_error_handling\n    result = func()\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 591, in code_to_exec\n    exec(code, module.__dict__)\n  File \"/kaggle/working/app.py\", line 12, in <module>\n    tokenizer,\nNameError: name 'tokenizer' is not defined\n2025-02-25 08:00:05.449 Examining the path of torch.classes raised:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/bootstrap.py\", line 345, in run\n    if asyncio.get_running_loop().is_running():\nRuntimeError: no running event loop\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n    potential_paths = extract_paths(module)\n  File \"/usr/local/lib/python3.10/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n    lambda m: list(m.__path__._path),\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_classes.py\", line 13, in __getattr__\n    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\nRuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!npm install localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:11:50.288309Z","iopub.execute_input":"2025-02-25T07:11:50.288662Z","iopub.status.idle":"2025-02-25T07:11:51.934458Z","shell.execute_reply.started":"2025-02-25T07:11:50.288616Z","shell.execute_reply":"2025-02-25T07:11:51.933520Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\nadded 22 packages in 1s\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"#Download model and tokenizer:\n!zip -r folder.zip /kaggle/working/Deepseek-r1-finetuned_health-8B","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:29:04.539194Z","iopub.execute_input":"2025-02-25T07:29:04.539553Z","iopub.status.idle":"2025-02-25T07:29:13.876121Z","shell.execute_reply.started":"2025-02-25T07:29:04.539527Z","shell.execute_reply":"2025-02-25T07:29:13.874953Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/Deepseek-r1-finetuned_health-8B/ (stored 0%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/tokenizer.json (deflated 85%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/special_tokens_map.json (deflated 65%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/README.md (deflated 66%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/adapter_config.json (deflated 56%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/adapter_model.safetensors (deflated 7%)\nupdating: kaggle/working/Deepseek-r1-finetuned_health-8B/tokenizer_config.json (deflated 94%)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'folder.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:33:04.617377Z","iopub.execute_input":"2025-02-25T07:33:04.617773Z","iopub.status.idle":"2025-02-25T07:33:04.623669Z","shell.execute_reply.started":"2025-02-25T07:33:04.617744Z","shell.execute_reply":"2025-02-25T07:33:04.622813Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/folder.zip","text/html":"<a href='folder.zip' target='_blank'>folder.zip</a><br>"},"metadata":{}}],"execution_count":45}]}